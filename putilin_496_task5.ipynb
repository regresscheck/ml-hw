{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125162780>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ffc5c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "\n",
    "# <Your architecture. Please start with a single-layer network>\n",
    "\n",
    "\n",
    "# nn = LSTMLayer(l_states, num_units=100, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "nn = DenseLayer(l_states, 100, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "nn = DenseLayer(nn, 50, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(nn,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], T.argmax(predicted_qvalues, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})\n",
    "\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma * T.max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#mean squared error loss function\n",
    "loss = lasagne.objectives.squared_error(predicted_qvalues_for_actions, target_qvalues_for_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.adam(loss.mean(),all_weights,learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states, actions, rewards, next_states, is_end],\n",
    "                             updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.2 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues(np.array([s],dtype=np.float32))[0] \n",
    "        rnd = np.random.uniform()\n",
    "        if rnd < epsilon:\n",
    "            a = np.random.choice(np.arange(n_actions))\n",
    "        else:\n",
    "            a = q_values\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step(np.array([s],dtype=np.float32),[a],[r],\n",
    "                   np.array([new_s],dtype=np.float32),[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import imp\n",
    "#imp.reload(tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\tmean reward:-198.588\tepsilon:0.19800\n",
      "epoch:1\tmean reward:-55.700\tepsilon:0.19602\n",
      "epoch:2\tmean reward:-31.258\tepsilon:0.19406\n",
      "epoch:3\tmean reward:47.016\tepsilon:0.19212\n",
      "epoch:4\tmean reward:65.618\tepsilon:0.19020\n",
      "epoch:5\tmean reward:102.427\tepsilon:0.18830\n",
      "epoch:6\tmean reward:91.653\tepsilon:0.18641\n",
      "epoch:7\tmean reward:88.338\tepsilon:0.18455\n",
      "epoch:8\tmean reward:81.706\tepsilon:0.18270\n",
      "epoch:9\tmean reward:106.223\tepsilon:0.18088\n",
      "epoch:10\tmean reward:88.207\tepsilon:0.17907\n",
      "epoch:11\tmean reward:107.025\tepsilon:0.17728\n",
      "epoch:12\tmean reward:109.400\tepsilon:0.17550\n"
     ]
    }
   ],
   "source": [
    "# t = tqdm.trange(1000)\n",
    "for i in range(1000):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.99\n",
    "#     t.set_postfix(mean_reward=np.mean(rewards), epsilon=epsilon)\n",
    "    print (\"epoch:%d\\tmean reward:%.3f\\tepsilon:%.5f\"%(i,np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_network(filename,param_values):\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(param_values,f)\n",
    "    f.close()\n",
    "save_network(\"network\", get_all_param_values(l_qvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_network(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    param_values = pickle.load(f)\n",
    "    f.close()\n",
    "    return param_values\n",
    "\n",
    "saved_params = load_network(\"model.npz\")\n",
    "lasagne.layers.set_all_param_values(network, saved_params)\n",
    "\n",
    "updates = lasagne.updates.adam(loss.mean(), network, learning_rate=1e-3)\n",
    "train_step = theano.function([current_states, actions, rewards, next_states, is_end],\n",
    "                             updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Tried to reset environment which is not done. While the monitor is active for LunarLander-v2, you cannot call reset() unless the episode is over.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-564a7da7ebc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#unwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-564a7da7ebc0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#unwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-75bf90aa9317>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for LunarLander-v2, you cannot call reset() unless the episode is over."
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
