{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросы для самопроверки:\n",
    "* что такое обучени с подкреплением (reinforcement learning)?\n",
    "* что такое среда?\n",
    "* что такое агент?\n",
    "* что такое награда, какая она может быть?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  FrozenLake\n",
    "Today you are going to learn how to survive walking over the (virtual) frozen lake through discrete optimization.\n",
    "\n",
    "<img src=\"http://vignette2.wikia.nocookie.net/riseoftheguardians/images/4/4c/Jack's_little_sister_on_the_ice.jpg/revision/latest?cb=20141218030206\" alt=\"a random image to attract attention\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-25 01:11:48,180] Making new env: Taxi-v2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "#start new game\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the game state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### legend\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/800/1*MCjDzR-wfMMkS0rPqXSmKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym interface\n",
    "\n",
    "The three main methods of an environment are\n",
    "* __reset()__ - reset environment to initial state, _return first observation_\n",
    "* __render()__ - show current environment state (a more colorful version :) )\n",
    "* __step(a)__ - commit action __a__ and return (new observation, reward, is done, info)\n",
    " * _new observation_ - an observation right after commiting the action __a__\n",
    " * _reward_ - a number representing your reward for commiting action __a__\n",
    " * _is done_ - True if the MDP has just finished, False if still in progress\n",
    " * _info_ - some auxilary stuff about what just happened. Ignore it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation code: 308\n",
      "printing observation:\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "observations: Discrete(500) n= 500\n",
      "actions: Discrete(6) n= 6\n"
     ]
    }
   ],
   "source": [
    "print(\"initial observation code:\", env.reset())\n",
    "print('printing observation:')\n",
    "env.render()\n",
    "print(\"observations:\", env.observation_space, 'n=', env.observation_space.n)\n",
    "print(\"actions:\", env.action_space, 'n=', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action 2 (right)\n",
      "new observation code: 308\n",
      "reward: -1\n",
      "is game over?: False\n",
      "printing new state:\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "source": [
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "print(\"printing new state:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_to_i = {\n",
    "    'right': 2,\n",
    "    'up': 1,\n",
    "    'down': 0,\n",
    "    'left': 3,\n",
    "    'pickup': 4,\n",
    "    'dropoff': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with it\n",
    "* Try walking 5 steps without falling to the (H)ole\n",
    " * Bonus quest - get to the (G)oal\n",
    "* Sometimes your actions will not be executed properly due to slipping over ice\n",
    "* If you fall, call __env.reset()__ to restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "env.step(action_to_i['left'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "env.step(action_to_i['up'])\n",
    "env.step(action_to_i['up'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "env.step(action_to_i['right'])\n",
    "env.step(action_to_i['up'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "* The environment has a 4x4 grid of states (16 total), they are indexed from 0 to 15\n",
    "* From each states there are 4 actions (left,down,right,up), indexed from 0 to 3\n",
    "\n",
    "We need to define agent's policy of picking actions given states. Since we have only 16 disttinct states and 4 actions, we can just store the action for each state in an array.\n",
    "\n",
    "This basically means that any array of 16 integers from 0 to 3 makes a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def get_random_policy():\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    return np.random.choice(range(n_actions), size=n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action frequencies over 10^4 samples: [ 0.1668264  0.1667818  0.166578   0.166587   0.166508   0.1667188]\n",
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [get_random_policy() for i in range(10**4)]\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should match n_actions-1'\n",
    "action_probas = np.unique(policies, return_counts=True)[-1] /10**4. /n_states\n",
    "print(\"Action frequencies over 10^4 samples:\",action_probas)\n",
    "assert np.allclose(action_probas, [1. / n_actions] * n_actions, atol=0.05), \"The policies aren't uniformly random (maybe it's just an extremely bad luck)\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate!\n",
    "* Implement a simple function that runs one game and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=100):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(t_max)\n",
    "        s, r, done, _ = env.step(policy[s])\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 10^3 sessions...\n",
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"generating 10^3 sessions...\")\n",
    "rewards = [sample_reward(env, get_random_policy()) for _ in range(1000)]\n",
    "assert all([type(r) in (int, float) for r in rewards]), 'sample_reward must return a single number'\n",
    "\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(policy, n_times=100):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    rewards = [sample_reward(env, policy) for i in range(n_times)]\n",
    "    return float(np.mean(rewards))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:00<29:04,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -592.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/10000 [00:00<23:47,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -511.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/10000 [00:02<19:11,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -476.38\n",
      "New best score: -448.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 184/10000 [00:15<13:54, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -440.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 242/10000 [00:20<13:28, 12.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -440.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 396/10000 [00:31<12:49, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -439.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 542/10000 [00:42<12:26, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -431.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 932/10000 [01:12<11:44, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -431.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2538/10000 [03:13<09:29, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -422.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2754/10000 [03:30<09:12, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -421.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 5628/10000 [07:07<05:32, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -412.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7400/10000 [09:22<03:17, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -405.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8096/10000 [10:14<02:24, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: -359.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [12:39<00:00, 13.17it/s]\n"
     ]
    }
   ],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10000)):\n",
    "    policy = get_random_policy()\n",
    "    score = evaluate(policy)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "        print(\"New best score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II Genetic algorithm \n",
    "\n",
    "The next task is to devise some more effecient way to perform policy search.\n",
    "We'll do that with a bare-bones evolutionary algorithm.\n",
    "[unless you're feeling masochistic and wish to do something entirely different which is bonus points if it works]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    n = len(policy1)\n",
    "    first = np.random.rand(n) < p\n",
    "    return policy1 * first + policy2 * (1 - first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutation(policy, p=0.1):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    return crossover(get_random_policy(), policy, p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [crossover(get_random_policy(), get_random_policy()) for i in range(10**4)]\n",
    "\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should be n_actions-1'\n",
    "\n",
    "assert any([np.mean(crossover(np.zeros(n_states), np.ones(n_states))) not in (0, 1)\n",
    "               for _ in range(100)]), \"Make sure your crossover changes each action independently\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 100 #how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n"
     ]
    }
   ],
   "source": [
    "print(\"initializing...\")\n",
    "pool = [get_random_policy() for _ in range(pool_size)]\n",
    "pool_scores = [evaluate(policy) for policy in pool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "best score: -413.47\n",
      "Epoch 1:\n",
      "best score: -440.02\n",
      "Epoch 2:\n",
      "best score: -440.29\n",
      "Epoch 3:\n",
      "best score: -405.1\n",
      "Epoch 4:\n",
      "best score: -413.38\n",
      "Epoch 5:\n",
      "best score: -393.94\n",
      "Epoch 6:\n",
      "best score: -350.83\n",
      "Epoch 7:\n",
      "best score: -323.65\n",
      "Epoch 8:\n",
      "best score: -359.56\n",
      "Epoch 9:\n",
      "best score: -342.1\n",
      "Epoch 10:\n",
      "best score: -306.01\n",
      "Epoch 11:\n",
      "best score: -341.2\n",
      "Epoch 12:\n",
      "best score: -324.19\n",
      "Epoch 13:\n",
      "best score: -341.02\n",
      "Epoch 14:\n",
      "best score: -323.83\n",
      "Epoch 15:\n",
      "best score: -260.83\n",
      "Epoch 16:\n",
      "best score: -287.92\n",
      "Epoch 17:\n",
      "best score: -279.37\n",
      "Epoch 18:\n",
      "best score: -296.92\n",
      "Epoch 19:\n",
      "best score: -278.74\n",
      "Epoch 20:\n",
      "best score: -278.11\n",
      "Epoch 21:\n",
      "best score: -288.1\n",
      "Epoch 22:\n",
      "best score: -279.55\n",
      "Epoch 23:\n",
      "best score: -279.55\n",
      "Epoch 24:\n",
      "best score: -252.28\n",
      "Epoch 25:\n",
      "best score: -261.46\n",
      "Epoch 26:\n",
      "best score: -243.46\n",
      "Epoch 27:\n",
      "best score: -261.37\n",
      "Epoch 28:\n",
      "best score: -260.83\n",
      "Epoch 29:\n",
      "best score: -261.28\n",
      "Epoch 30:\n",
      "best score: -252.28\n",
      "Epoch 31:\n",
      "best score: -225.64\n",
      "Epoch 32:\n",
      "best score: -207.46\n",
      "Epoch 33:\n",
      "best score: -243.55\n",
      "Epoch 34:\n",
      "best score: -215.83\n",
      "Epoch 35:\n",
      "best score: -216.55\n",
      "Epoch 36:\n",
      "best score: -216.46\n",
      "Epoch 37:\n",
      "best score: -198.55\n",
      "Epoch 38:\n",
      "best score: -198.73\n",
      "Epoch 39:\n",
      "best score: -207.73\n",
      "Epoch 40:\n",
      "best score: -207.19\n",
      "Epoch 41:\n",
      "best score: -198.82\n",
      "Epoch 42:\n",
      "best score: -207.37\n",
      "Epoch 43:\n",
      "best score: -189.73\n",
      "Epoch 44:\n",
      "best score: -189.64\n",
      "Epoch 45:\n",
      "best score: -189.73\n",
      "Epoch 46:\n",
      "best score: -171.55\n",
      "Epoch 47:\n",
      "best score: -180.64\n",
      "Epoch 48:\n",
      "best score: -189.64\n",
      "Epoch 49:\n",
      "best score: -153.82\n",
      "Epoch 50:\n",
      "best score: -153.82\n",
      "Epoch 51:\n",
      "best score: -171.73\n",
      "Epoch 52:\n",
      "best score: -171.55\n",
      "Epoch 53:\n",
      "best score: -144.73\n",
      "Epoch 54:\n",
      "best score: -153.46\n",
      "Epoch 55:\n",
      "best score: -162.73\n",
      "Epoch 56:\n",
      "best score: -144.55\n",
      "Epoch 57:\n",
      "best score: -144.82\n",
      "Epoch 58:\n",
      "best score: -153.73\n",
      "Epoch 59:\n",
      "best score: -144.73\n",
      "Epoch 60:\n",
      "best score: -136.0\n",
      "Epoch 61:\n",
      "best score: -145.0\n",
      "Epoch 62:\n",
      "best score: -144.82\n",
      "Epoch 63:\n",
      "best score: -144.82\n",
      "Epoch 64:\n",
      "best score: -135.91\n",
      "Epoch 65:\n",
      "best score: -135.91\n",
      "Epoch 66:\n",
      "best score: -126.91\n",
      "Epoch 67:\n",
      "best score: -118.0\n",
      "Epoch 68:\n",
      "best score: -117.82\n",
      "Epoch 69:\n",
      "best score: -135.82\n",
      "Epoch 70:\n",
      "best score: -117.91\n",
      "Epoch 71:\n",
      "best score: -109.0\n",
      "Epoch 72:\n",
      "best score: -108.91\n",
      "Epoch 73:\n",
      "best score: -109.0\n",
      "Epoch 74:\n",
      "best score: -117.82\n",
      "Epoch 75:\n",
      "best score: -118.0\n",
      "Epoch 76:\n",
      "best score: -117.82\n",
      "Epoch 77:\n",
      "best score: -109.0\n",
      "Epoch 78:\n",
      "best score: -109.0\n",
      "Epoch 79:\n",
      "best score: -100.0\n",
      "Epoch 80:\n",
      "best score: -100.0\n",
      "Epoch 81:\n",
      "best score: -108.91\n",
      "Epoch 82:\n",
      "best score: -100.0\n",
      "Epoch 83:\n",
      "best score: -100.0\n",
      "Epoch 84:\n",
      "best score: -100.0\n",
      "Epoch 85:\n",
      "best score: -100.0\n",
      "Epoch 86:\n",
      "best score: -100.0\n",
      "Epoch 87:\n",
      "best score: -100.0\n",
      "Epoch 88:\n",
      "best score: -100.0\n",
      "Epoch 89:\n",
      "best score: -100.0\n",
      "Epoch 90:\n",
      "best score: -100.0\n",
      "Epoch 91:\n",
      "best score: -100.0\n",
      "Epoch 92:\n",
      "best score: -100.0\n",
      "Epoch 93:\n",
      "best score: -100.0\n",
      "Epoch 94:\n",
      "best score: -100.0\n",
      "Epoch 95:\n",
      "best score: -100.0\n",
      "Epoch 96:\n",
      "best score: -100.0\n",
      "Epoch 97:\n",
      "best score: -100.0\n",
      "Epoch 98:\n",
      "best score: -100.0\n",
      "Epoch 99:\n",
      "best score: -100.0\n"
     ]
    }
   ],
   "source": [
    "#main loop\n",
    "for epoch in range(n_epochs):    \n",
    "    crossovered = [crossover(pool[np.random.randint(pool_size)],\n",
    "                             pool[np.random.randint(pool_size)])\n",
    "                   for _ in range(n_crossovers)]\n",
    "    mutated = [mutation(pool[np.random.randint(pool_size)])\n",
    "               for _ in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    #add new policies to the pool\n",
    "    pool.extend(crossovered)\n",
    "    pool.extend(mutated)\n",
    "    \n",
    "    pool_scores = [evaluate(policy) for policy in pool]\n",
    "    \n",
    "    #select pool_size best policies\n",
    "    selected_indices = np.argsort(pool_scores)[-pool_size:]\n",
    "    pool = [pool[i] for i in selected_indices]\n",
    "    pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "    #print the best policy so far (last in ascending score order)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\"Epoch\", epoch)\n",
    "        print(\"best score:\", pool_scores[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на фидбек по семинару: [link](https://docs.google.com/forms/d/e/1FAIpQLSf-08wFrEke6zKlysETYiqAjH5CRXtOKut5Q77Tr5rdVId7zA/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, learning_rate, epsilon, discount, n_actions):\n",
    "        self.actions= range(n_actions)\n",
    "        self.qvalues = defaultdict(lambda : defaultdict(float))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        return self.actions[np.argmax([self.qvalues[state][action] for action in self.actions])]\n",
    "\n",
    "    def update(self, state, action, next_state, reward):\n",
    "        value = np.max([self.qvalues[next_state][action] for action in self.actions])\n",
    "        reference_qvalue = self.discount * value + reward\n",
    "        updated_qvalue = (1 - self.learning_rate) * self.qvalues[state][action] + self.learning_rate * reference_qvalue\n",
    "        self.qvalues[state][action] = updated_qvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99. Mean reward over last 300 steps: -258.49\n",
      "199. Mean reward over last 300 steps: -183.185\n",
      "299. Mean reward over last 300 steps: -130.34333333333333\n",
      "399. Mean reward over last 300 steps: -47.22666666666667\n",
      "499. Mean reward over last 300 steps: -10.26\n",
      "599. Mean reward over last 300 steps: -0.77\n",
      "699. Mean reward over last 300 steps: 4.366666666666666\n",
      "799. Mean reward over last 300 steps: 5.693333333333333\n",
      "899. Mean reward over last 300 steps: 6.863333333333333\n",
      "999. Mean reward over last 300 steps: 7.43\n",
      "\n",
      "\n",
      "1099. Mean reward over last 300 steps: 7.886666666666667\n",
      "1199. Mean reward over last 300 steps: 8.09\n",
      "1299. Mean reward over last 300 steps: 8.13\n",
      "1399. Mean reward over last 300 steps: 7.993333333333333\n",
      "1499. Mean reward over last 300 steps: 8.206666666666667\n",
      "1599. Mean reward over last 300 steps: 8.323333333333334\n",
      "1699. Mean reward over last 300 steps: 8.453333333333333\n",
      "1799. Mean reward over last 300 steps: 8.413333333333334\n",
      "1899. Mean reward over last 300 steps: 8.306666666666667\n",
      "1999. Mean reward over last 300 steps: 8.2\n",
      "\n",
      "\n",
      "2099. Mean reward over last 300 steps: 8.176666666666666\n",
      "2199. Mean reward over last 300 steps: 8.186666666666667\n",
      "2299. Mean reward over last 300 steps: 8.206666666666667\n",
      "2399. Mean reward over last 300 steps: 8.196666666666667\n",
      "2499. Mean reward over last 300 steps: 8.196666666666667\n",
      "2599. Mean reward over last 300 steps: 8.233333333333333\n",
      "2699. Mean reward over last 300 steps: 8.016666666666667\n",
      "2799. Mean reward over last 300 steps: 7.91\n",
      "2899. Mean reward over last 300 steps: 7.806666666666667\n",
      "2999. Mean reward over last 300 steps: 7.75\n",
      "\n",
      "\n",
      "3099. Mean reward over last 300 steps: 7.746666666666667\n",
      "3199. Mean reward over last 300 steps: 7.85\n",
      "3299. Mean reward over last 300 steps: 8.076666666666666\n",
      "3399. Mean reward over last 300 steps: 7.986666666666666\n",
      "3499. Mean reward over last 300 steps: 7.983333333333333\n",
      "3599. Mean reward over last 300 steps: 7.98\n",
      "3699. Mean reward over last 300 steps: 8.2\n",
      "3799. Mean reward over last 300 steps: 8.113333333333333\n",
      "3899. Mean reward over last 300 steps: 7.973333333333334\n",
      "3999. Mean reward over last 300 steps: 7.976666666666667\n",
      "\n",
      "\n",
      "4099. Mean reward over last 300 steps: 8.09\n",
      "4199. Mean reward over last 300 steps: 8.21\n",
      "4299. Mean reward over last 300 steps: 8.223333333333333\n",
      "4399. Mean reward over last 300 steps: 8.19\n",
      "4499. Mean reward over last 300 steps: 8.023333333333333\n",
      "4599. Mean reward over last 300 steps: 7.84\n",
      "4699. Mean reward over last 300 steps: 7.92\n",
      "4799. Mean reward over last 300 steps: 7.98\n",
      "4899. Mean reward over last 300 steps: 8.246666666666666\n",
      "4999. Mean reward over last 300 steps: 8.036666666666667\n",
      "\n",
      "\n",
      "5099. Mean reward over last 300 steps: 8.246666666666666\n",
      "5199. Mean reward over last 300 steps: 7.983333333333333\n",
      "5299. Mean reward over last 300 steps: 8.096666666666666\n",
      "5399. Mean reward over last 300 steps: 8.05\n",
      "5499. Mean reward over last 300 steps: 8.296666666666667\n",
      "5599. Mean reward over last 300 steps: 8.4\n",
      "5699. Mean reward over last 300 steps: 8.286666666666667\n",
      "5799. Mean reward over last 300 steps: 8.103333333333333\n",
      "5899. Mean reward over last 300 steps: 8.06\n",
      "5999. Mean reward over last 300 steps: 8.226666666666667\n",
      "\n",
      "\n",
      "6099. Mean reward over last 300 steps: 8.2\n",
      "6199. Mean reward over last 300 steps: 8.083333333333334\n",
      "6299. Mean reward over last 300 steps: 8.003333333333334\n",
      "6399. Mean reward over last 300 steps: 8.113333333333333\n",
      "6499. Mean reward over last 300 steps: 8.146666666666667\n",
      "6599. Mean reward over last 300 steps: 8.07\n",
      "6699. Mean reward over last 300 steps: 8.056666666666667\n",
      "6799. Mean reward over last 300 steps: 7.88\n",
      "6899. Mean reward over last 300 steps: 7.9366666666666665\n",
      "6999. Mean reward over last 300 steps: 7.88\n",
      "\n",
      "\n",
      "7099. Mean reward over last 300 steps: 7.986666666666666\n",
      "7199. Mean reward over last 300 steps: 7.97\n",
      "7299. Mean reward over last 300 steps: 8.01\n",
      "7399. Mean reward over last 300 steps: 8.086666666666666\n",
      "7499. Mean reward over last 300 steps: 7.943333333333333\n",
      "7599. Mean reward over last 300 steps: 8.09\n",
      "7699. Mean reward over last 300 steps: 7.993333333333333\n",
      "7799. Mean reward over last 300 steps: 8.233333333333333\n",
      "7899. Mean reward over last 300 steps: 8.003333333333334\n",
      "7999. Mean reward over last 300 steps: 8.15\n",
      "\n",
      "\n",
      "8099. Mean reward over last 300 steps: 7.923333333333333\n",
      "8199. Mean reward over last 300 steps: 8.056666666666667\n",
      "8299. Mean reward over last 300 steps: 7.973333333333334\n",
      "8399. Mean reward over last 300 steps: 8.236666666666666\n",
      "8499. Mean reward over last 300 steps: 8.156666666666666\n",
      "8599. Mean reward over last 300 steps: 8.216666666666667\n",
      "8699. Mean reward over last 300 steps: 8.06\n",
      "8799. Mean reward over last 300 steps: 8.123333333333333\n",
      "8899. Mean reward over last 300 steps: 8.1\n",
      "8999. Mean reward over last 300 steps: 7.966666666666667\n",
      "\n",
      "\n",
      "9099. Mean reward over last 300 steps: 7.97\n",
      "9199. Mean reward over last 300 steps: 8.036666666666667\n",
      "9299. Mean reward over last 300 steps: 8.186666666666667\n",
      "9399. Mean reward over last 300 steps: 8.243333333333334\n",
      "9499. Mean reward over last 300 steps: 8.286666666666667\n",
      "9599. Mean reward over last 300 steps: 8.3\n",
      "9699. Mean reward over last 300 steps: 8.236666666666666\n",
      "9799. Mean reward over last 300 steps: 8.1\n",
      "9899. Mean reward over last 300 steps: 8.173333333333334\n",
      "9999. Mean reward over last 300 steps: 8.193333333333333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(discount=0.99, learning_rate=0.3, epsilon=0.1, n_actions=6)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for i in range(10**4):\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(10**4):\n",
    "        a = agent.act(s)\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, new_s, r)\n",
    "        s = new_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    agent.epsilon *= 0.995\n",
    "\n",
    "    if (i + 1) % 100 ==0:\n",
    "        print(\"{}. Mean reward over last 300 steps: {}\\r\".format(i, np.mean(rewards[-300:])))\n",
    "    if (i + 1) % 1000 ==0:\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
